{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b81ba3bc",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pyLDAvis.sklearn'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/ks/m1zw93_s1xbbylb_bjzqb3z80000gn/T/ipykernel_69733/2131316629.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_extraction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCountVectorizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecomposition\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLatentDirichletAllocation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mpyLDAvis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msklearn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpyLDAvis\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pyLDAvis.sklearn'"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "from collections import Counter\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from textblob import TextBlob\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "import pyLDAvis.sklearn\n",
    "import pyLDAvis\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "print(\"scikit-learn is functional!\")\n",
    "\n",
    "# Initialize sentiment analyzers\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Load the processed dataset\n",
    "file_path = '/Users/dinabandhupanigrahi/customer-sentiment-dashboard/data/processed/Musical_Instruments_cleaned.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Precompute additional variables\n",
    "df['review_length'] = df['reviewText'].fillna('').apply(len)  # Review length\n",
    "df['reviewText'] = df['reviewText'].fillna('').astype(str)    # Ensure text is string\n",
    "\n",
    "# Sentiment Scores and Classification\n",
    "df['sentiment_score'] = df['reviewText'].apply(lambda x: sid.polarity_scores(x)['compound'])\n",
    "df['sentiment'] = df['sentiment_score'].apply(\n",
    "    lambda x: 'positive' if x > 0.05 else 'negative' if x < -0.05 else 'neutral'\n",
    ")\n",
    "\n",
    "# Filter negative reviews\n",
    "negative_reviews = df[df['sentiment'] == 'negative'].copy()\n",
    "\n",
    "# Vectorize text for clustering and topic modeling\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=500, stop_words='english')\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(negative_reviews['reviewText'])\n",
    "\n",
    "\n",
    "# Vectorize the text data for topic modeling\n",
    "count_vectorizer = CountVectorizer(max_df=0.9, min_df=2, stop_words='english')\n",
    "count_matrix = count_vectorizer.fit_transform(negative_reviews['reviewText'])\n",
    "\n",
    "# Perform LDA\n",
    "num_topics = 3  # You can adjust this based on your dataset\n",
    "lda = LatentDirichletAllocation(n_components=num_topics, random_state=42)\n",
    "lda.fit(count_matrix)\n",
    "\n",
    "# Display top words for each topic\n",
    "terms = count_vectorizer.get_feature_names_out()\n",
    "print(\"\\nTop Words for Each Topic:\")\n",
    "for idx, topic in enumerate(lda.components_):\n",
    "    top_terms = [terms[i] for i in topic.argsort()[-10:][::-1]]\n",
    "    print(f\"Topic {idx + 1}: {', '.join(top_terms)}\")\n",
    "\n",
    "# Assign topics to reviews\n",
    "negative_reviews['topic'] = lda.transform(count_matrix).argmax(axis=1)\n",
    "\n",
    "# Analyze topic distribution\n",
    "print(\"\\nTopic Distribution Across Reviews:\")\n",
    "print(negative_reviews['topic'].value_counts(normalize=True))\n",
    "\n",
    "# Visualize topics with pyLDAvis\n",
    "pyLDAvis.enable_notebook()\n",
    "vis = pyLDAvis.sklearn.prepare(lda, count_matrix, count_vectorizer)\n",
    "pyLDAvis.save_html(vis, 'topic_modeling_vis.html')\n",
    "print(\"Interactive topic modeling visualization saved as 'topic_modeling_vis.html'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b2d532e6",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pyLDAvis.sklearn'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/ks/m1zw93_s1xbbylb_bjzqb3z80000gn/T/ipykernel_69733/2919960736.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcollections\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCounter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpyLDAvis\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mpyLDAvis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msklearn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pyLDAvis.sklearn'"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from collections import Counter\n",
    "import pyLDAvis\n",
    "import pyLDAvis.sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Verify scikit-learn functionality\n",
    "print(\"scikit-learn is functional!\")\n",
    "\n",
    "# Initialize sentiment analyzer\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Load the processed dataset\n",
    "file_path = '/Users/dinabandhupanigrahi/customer-sentiment-dashboard/data/processed/Musical_Instruments_cleaned.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Precompute additional variables\n",
    "df['review_length'] = df['reviewText'].fillna('').apply(len)  # Review length\n",
    "df['reviewText'] = df['reviewText'].fillna('').astype(str)    # Ensure text is string\n",
    "\n",
    "# Sentiment Scores and Classification\n",
    "df['sentiment_score'] = df['reviewText'].apply(lambda x: sid.polarity_scores(x)['compound'])\n",
    "df['sentiment'] = df['sentiment_score'].apply(\n",
    "    lambda x: 'positive' if x > 0.05 else 'negative' if x < -0.05 else 'neutral'\n",
    ")\n",
    "\n",
    "# Filter negative reviews\n",
    "negative_reviews = df[df['sentiment'] == 'negative'].copy()\n",
    "\n",
    "# Vectorize the text data for topic modeling\n",
    "count_vectorizer = CountVectorizer(max_df=0.9, min_df=2, stop_words='english')\n",
    "count_matrix = count_vectorizer.fit_transform(negative_reviews['reviewText'])\n",
    "\n",
    "# Perform LDA\n",
    "num_topics = 3  # Adjust based on dataset\n",
    "lda = LatentDirichletAllocation(n_components=num_topics, random_state=42)\n",
    "lda.fit(count_matrix)\n",
    "\n",
    "# Display top words for each topic\n",
    "terms = count_vectorizer.get_feature_names_out()\n",
    "print(\"\\nTop Words for Each Topic:\")\n",
    "for idx, topic in enumerate(lda.components_):\n",
    "    top_terms = [terms[i] for i in topic.argsort()[-10:][::-1]]\n",
    "    print(f\"Topic {idx + 1}: {', '.join(top_terms)}\")\n",
    "\n",
    "# Assign topics to reviews\n",
    "negative_reviews['topic'] = lda.transform(count_matrix).argmax(axis=1)\n",
    "\n",
    "# Analyze topic distribution\n",
    "print(\"\\nTopic Distribution Across Reviews:\")\n",
    "print(negative_reviews['topic'].value_counts(normalize=True))\n",
    "\n",
    "# Visualize topics with pyLDAvis\n",
    "pyLDAvis.enable_notebook()\n",
    "vis = pyLDAvis.sklearn.prepare(lda, count_matrix, count_vectorizer)\n",
    "pyLDAvis.save_html(vis, 'topic_modeling_vis.html')\n",
    "print(\"Interactive topic modeling visualization saved as 'topic_modeling_vis.html'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f339021a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scikit-learn is functional!\n",
      "\n",
      "Top Words for Each Topic:\n",
      "Topic 1: amp, power, just, really, dont, strap, like, quality, screws, im\n",
      "Topic 2: pedal, guitar, strap, use, just, sound, like, dont, price, good\n",
      "Topic 3: guitar, strings, just, use, like, good, does, dont, string, sound\n",
      "\n",
      "Topic Distribution Across Reviews:\n",
      "2    0.539175\n",
      "1    0.278351\n",
      "0    0.182474\n",
      "Name: topic, dtype: float64\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "prepare() missing 2 required positional arguments: 'vocab' and 'term_frequency'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/ks/m1zw93_s1xbbylb_bjzqb3z80000gn/T/ipykernel_69733/547188335.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;31m# Visualize topics with pyLDAvis\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0mpyLDAvis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_notebook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m \u001b[0mvis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpyLDAvis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprepare\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlda\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcount_matrix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcount_vectorizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m \u001b[0mpyLDAvis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_html\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'topic_modeling_vis.html'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interactive topic modeling visualization saved as 'topic_modeling_vis.html'.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: prepare() missing 2 required positional arguments: 'vocab' and 'term_frequency'"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from collections import Counter\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Verify scikit-learn functionality\n",
    "print(\"scikit-learn is functional!\")\n",
    "\n",
    "# Initialize sentiment analyzer\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Load the processed dataset\n",
    "file_path = '/Users/dinabandhupanigrahi/customer-sentiment-dashboard/data/processed/Musical_Instruments_cleaned.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Precompute additional variables\n",
    "df['review_length'] = df['reviewText'].fillna('').apply(len)  # Review length\n",
    "df['reviewText'] = df['reviewText'].fillna('').astype(str)    # Ensure text is string\n",
    "\n",
    "# Sentiment Scores and Classification\n",
    "df['sentiment_score'] = df['reviewText'].apply(lambda x: sid.polarity_scores(x)['compound'])\n",
    "df['sentiment'] = df['sentiment_score'].apply(\n",
    "    lambda x: 'positive' if x > 0.05 else 'negative' if x < -0.05 else 'neutral'\n",
    ")\n",
    "\n",
    "# Filter negative reviews\n",
    "negative_reviews = df[df['sentiment'] == 'negative'].copy()\n",
    "\n",
    "# Vectorize the text data for topic modeling\n",
    "count_vectorizer = CountVectorizer(max_df=0.9, min_df=2, stop_words='english')\n",
    "count_matrix = count_vectorizer.fit_transform(negative_reviews['reviewText'])\n",
    "\n",
    "# Perform LDA\n",
    "num_topics = 3  # Adjust based on dataset\n",
    "lda = LatentDirichletAllocation(n_components=num_topics, random_state=42)\n",
    "lda.fit(count_matrix)\n",
    "\n",
    "# Display top words for each topic\n",
    "terms = count_vectorizer.get_feature_names_out()\n",
    "print(\"\\nTop Words for Each Topic:\")\n",
    "for idx, topic in enumerate(lda.components_):\n",
    "    top_terms = [terms[i] for i in topic.argsort()[-10:][::-1]]\n",
    "    print(f\"Topic {idx + 1}: {', '.join(top_terms)}\")\n",
    "\n",
    "# Assign topics to reviews\n",
    "negative_reviews['topic'] = lda.transform(count_matrix).argmax(axis=1)\n",
    "\n",
    "# Analyze topic distribution\n",
    "print(\"\\nTopic Distribution Across Reviews:\")\n",
    "print(negative_reviews['topic'].value_counts(normalize=True))\n",
    "\n",
    "# Visualize topics with pyLDAvis\n",
    "pyLDAvis.enable_notebook()\n",
    "vis = pyLDAvis.prepare(lda, count_matrix, count_vectorizer)\n",
    "pyLDAvis.save_html(vis, 'topic_modeling_vis.html')\n",
    "print(\"Interactive topic modeling visualization saved as 'topic_modeling_vis.html'.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "54e28246",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pyLDAvis.sklearn'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/ks/m1zw93_s1xbbylb_bjzqb3z80000gn/T/ipykernel_69733/1330998012.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcollections\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCounter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpyLDAvis\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mpyLDAvis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msklearn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pyLDAvis.sklearn'"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from collections import Counter\n",
    "import pyLDAvis\n",
    "import pyLDAvis.sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Verify scikit-learn functionality\n",
    "print(\"scikit-learn is functional!\")\n",
    "\n",
    "# Initialize sentiment analyzer\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Load the processed dataset\n",
    "file_path = '/Users/dinabandhupanigrahi/customer-sentiment-dashboard/data/processed/Musical_Instruments_cleaned.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Precompute additional variables\n",
    "df['review_length'] = df['reviewText'].fillna('').apply(len)  # Review length\n",
    "df['reviewText'] = df['reviewText'].fillna('').astype(str)    # Ensure text is string\n",
    "\n",
    "# Sentiment Scores and Classification\n",
    "df['sentiment_score'] = df['reviewText'].apply(lambda x: sid.polarity_scores(x)['compound'])\n",
    "df['sentiment'] = df['sentiment_score'].apply(\n",
    "    lambda x: 'positive' if x > 0.05 else 'negative' if x < -0.05 else 'neutral'\n",
    ")\n",
    "\n",
    "# Filter negative reviews\n",
    "negative_reviews = df[df['sentiment'] == 'negative'].copy()\n",
    "\n",
    "# Vectorize the text data for topic modeling\n",
    "count_vectorizer = CountVectorizer(max_df=0.9, min_df=2, stop_words='english')\n",
    "count_matrix = count_vectorizer.fit_transform(negative_reviews['reviewText'])\n",
    "\n",
    "# Perform LDA\n",
    "num_topics = 3  # Adjust based on dataset\n",
    "lda = LatentDirichletAllocation(n_components=num_topics, random_state=42)\n",
    "lda.fit(count_matrix)\n",
    "\n",
    "# Display top words for each topic\n",
    "terms = count_vectorizer.get_feature_names_out()\n",
    "print(\"\\nTop Words for Each Topic:\")\n",
    "for idx, topic in enumerate(lda.components_):\n",
    "    top_terms = [terms[i] for i in topic.argsort()[-10:][::-1]]\n",
    "    print(f\"Topic {idx + 1}: {', '.join(top_terms)}\")\n",
    "\n",
    "# Assign topics to reviews\n",
    "negative_reviews['topic'] = lda.transform(count_matrix).argmax(axis=1)\n",
    "\n",
    "# Analyze topic distribution\n",
    "print(\"\\nTopic Distribution Across Reviews:\")\n",
    "print(negative_reviews['topic'].value_counts(normalize=True))\n",
    "\n",
    "# Visualize topics with pyLDAvis\n",
    "pyLDAvis.enable_notebook()\n",
    "\n",
    "# Calculate term frequencies\n",
    "term_frequency = count_matrix.sum(axis=0).A1\n",
    "vocab = count_vectorizer.get_feature_names_out()\n",
    "\n",
    "# Prepare visualization\n",
    "vis = pyLDAvis.prepare(lda.components_, count_matrix, len(negative_reviews), vocab, term_frequency)\n",
    "pyLDAvis.save_html(vis, 'topic_modeling_vis.html')\n",
    "print(\"Interactive topic modeling visualization saved as 'topic_modeling_vis.html'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c5715d55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scikit-learn is functional!\n",
      "\n",
      "Top Words for Each Topic:\n",
      "Topic 1: amp, power, just, really, dont, strap, like, quality, screws, im\n",
      "Topic 2: pedal, guitar, strap, use, just, sound, like, dont, price, good\n",
      "Topic 3: guitar, strings, just, use, like, good, does, dont, string, sound\n",
      "\n",
      "Topic Distribution Across Reviews:\n",
      "2    0.539175\n",
      "1    0.278351\n",
      "0    0.182474\n",
      "Name: topic, dtype: float64\n"
     ]
    },
    {
     "ename": "BrokenProcessPool",
     "evalue": "A task has failed to un-serialize. Please ensure that the arguments of the function are all picklable.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31m_RemoteTraceback\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;31m_RemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"/Users/dinabandhupanigrahi/opt/anaconda3/lib/python3.9/site-packages/joblib/externals/loky/process_executor.py\", line 426, in _process_worker\n    call_item = call_queue.get(block=True, timeout=timeout)\n  File \"/Users/dinabandhupanigrahi/opt/anaconda3/lib/python3.9/multiprocessing/queues.py\", line 122, in get\n    return _ForkingPickler.loads(res)\nAttributeError: Can't get attribute 'SafeFunction' on <module 'joblib._parallel_backends' from '/Users/dinabandhupanigrahi/opt/anaconda3/lib/python3.9/site-packages/joblib/_parallel_backends.py'>\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mBrokenProcessPool\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/ks/m1zw93_s1xbbylb_bjzqb3z80000gn/T/ipykernel_69733/3000657421.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;31m# Prepare visualization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m vis = pyLDAvis.prepare(\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0mtopic_term_dists\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcomponents_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0mdoc_topic_dists\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcount_matrix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pyLDAvis/_prepare.py\u001b[0m in \u001b[0;36mprepare\u001b[0;34m(topic_term_dists, doc_topic_dists, doc_lengths, vocab, term_frequency, R, lambda_step, mds, n_jobs, plot_opts, sort_topics, start_index)\u001b[0m\n\u001b[1;32m    430\u001b[0m     \u001b[0mterm_frequency\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mterm_topic_freq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    431\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 432\u001b[0;31m     topic_info = _topic_info(topic_term_dists, topic_proportion,\n\u001b[0m\u001b[1;32m    433\u001b[0m                              \u001b[0mterm_frequency\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mterm_topic_freq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlambda_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mR\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m                              n_jobs, start_index)\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pyLDAvis/_prepare.py\u001b[0m in \u001b[0;36m_topic_info\u001b[0;34m(topic_term_dists, topic_proportion, term_frequency, term_topic_freq, vocab, lambda_step, R, n_jobs, start_index)\u001b[0m\n\u001b[1;32m    271\u001b[0m         ])\n\u001b[1;32m    272\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 273\u001b[0;31m     top_terms = pd.concat(Parallel(n_jobs=n_jobs)\n\u001b[0m\u001b[1;32m    274\u001b[0m                           (delayed(_find_relevance_chunks)(log_ttd, log_lift, R, ls)\n\u001b[1;32m    275\u001b[0m                           for ls in _job_chunks(lambda_seq, n_jobs)))\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1054\u001b[0m               \u001b[0mvariable\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1055\u001b[0m             \u001b[0;34m-\u001b[0m \u001b[0;34m/\u001b[0m\u001b[0mdev\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mshm\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mfolder\u001b[0m \u001b[0mexists\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mwritable\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mthis\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1056\u001b[0;31m               \u001b[0mRAM\u001b[0m \u001b[0mdisk\u001b[0m \u001b[0mfilesystem\u001b[0m \u001b[0mavailable\u001b[0m \u001b[0mby\u001b[0m \u001b[0mdefault\u001b[0m \u001b[0mon\u001b[0m \u001b[0mmodern\u001b[0m \u001b[0mLinux\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1057\u001b[0m               \u001b[0mdistributions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1058\u001b[0m             \u001b[0;34m-\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mdefault\u001b[0m \u001b[0msystem\u001b[0m \u001b[0mtemporary\u001b[0m \u001b[0mfolder\u001b[0m \u001b[0mthat\u001b[0m \u001b[0mcan\u001b[0m \u001b[0mbe\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    933\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    934\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mversionadded\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m0.10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 935\u001b[0;31m     \"\"\"\n\u001b[0m\u001b[1;32m    936\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mn_jobs\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    937\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mwrap_future_result\u001b[0;34m(future, timeout)\u001b[0m\n\u001b[1;32m    540\u001b[0m             \u001b[0menv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_prepare_worker_env\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    541\u001b[0m             context_id=parallel._id, **memmappingexecutor_args)\n\u001b[0;32m--> 542\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparallel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparallel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    543\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/concurrent/futures/_base.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    444\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mCancelledError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    445\u001b[0m                 \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mFINISHED\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 446\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    447\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/concurrent/futures/_base.py\u001b[0m in \u001b[0;36m__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    389\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    390\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 391\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    392\u001b[0m             \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    393\u001b[0m                 \u001b[0;31m# Break a reference cycle with the exception in self._exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mBrokenProcessPool\u001b[0m: A task has failed to un-serialize. Please ensure that the arguments of the function are all picklable."
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e609bab2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
